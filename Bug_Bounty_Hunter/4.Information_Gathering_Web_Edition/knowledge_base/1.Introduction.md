# Introduction

The primary goals of web reconnaissance is:
- **Identifying Assets:** Discover all publicly accessible components, such as web pages, subdomains, IP addresses, technologies etc.
- **Discovering Hidden Information:** Backup files, configuration files, internal documentation etc.
- **Analysing the Attack Surface:** Examine the attack surface to identify potential vulnerabilities and weaknesses. Technologies used, configurations, and possible entry points for exploitation.
- **Gathering Intelligence:** Information taht can be leveraged for further exploitation or social engineering, such as identifying key personnel, email addresses, or patterns of behaviour that can be exploited.

## Types of Reconnaissance

There are two methodologies:
- Active
- Passive

### Active Reconnaissance

This happens when the attacker directly interacts with the target system.

**Port Scanning**
Open ports and services running on the target.
Tools: `nmap, masscan, unicornscan`.
Risk of Detection: High - triggers IDS and firewalls.

**Vulnerability Scanning**
Known vulnerabilities such as outdated software or misconfiguration.
Tools: `Nessus, OpenVas, Nikto`.
Risk of Detection: High - Send exploit payloads that security solutions can detect.

**Network Mapping**
Mapping the network topology, connected devices and relationships.
Tools: `traceroute, nmap`.
Risk of Detection: Medium to High - Unusual network traffic raises suspicion.

**Banner Grabbing**
Retrieves information from banners displayed by services running.
Tools: `Netcat, curl`.
Risk Detection: Low - Minimal interaction, but can be logged.

**OS Fingerpriting**
Identify the OS.
Tools: `nmap, xprobe2`.
Risk Detection: Low - Is usually passive, but advanced techniques can be detected.

**Service Enumeration**
Determine specific versions of services running on open ports.
Tools: `nmap`.
Risk Detection: Low - Can be logged but is less likely to trigger alerts - similar to OS fingerprinting.

**Web Spidering**
Crawling the website to identify web pages, directories, and files.
Tools: `Burp Suite Spider, OWASP ZAP Spider, Scrapy`
Risk Detection: Low to Medium - Can be detected if the behaviour is not carefully configured to mimic legitimate traffic.

### Passive Reconnaissance

Gathering information without directly interacting with it.

**Search Engine Queries**
Uncover information such as websites, social media profiles, and news articles.
Tools: `Google, DuckDuckGo, Shodan, etc`.
Risk Detection: Very Low - Normal internet activities.

**Whois Lookups**
Retreive domain registration details.
Tools: `whois, online WHOIS lookup services`.
Risk Detection: Very Low.

**DNS**
Analysing DNS records to identify subdomains, mail servers, other infrastructure.
Tools: `dig, nslookup, host, dnsenum, fierce, dnsrecon`.
Risk Detection: Very Low.

**Web Archive Analysis**
Examines historical snapshots of the website to identify changes, culnerabilities, or hidden information.
Tools: `Wayback Machine`.
Risk Detection: Very Low.

**Social Media Analysis**
Tools: `Linkedin, twitter, Facebook, specialized OSINT tools`.
Risk Detection: Very Low.

**Code Repositories**
Analysing publicly accessible code repositories for credentials or vulnerabilities.
Tools: `GitHub, GitLab`.
Risk Detection: Very Low.

