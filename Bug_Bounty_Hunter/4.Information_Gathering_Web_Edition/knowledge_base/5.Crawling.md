# Crawling

**Crawling** or **Spidering** is the automated process of systematically browsing the WWW.

**How does it work**

It starts with an URL and fetches the page, extracts its link, adds them to the queue and crawls them, and repeats this process.

**Breadth-First Crawling**

Prioritizes the website's width before going deep, meaning that it starts by crawling all the links then moves on to the links on those pages.

Useful for getting a broad overview of a website's structure and content.

**Depth-First Crawling**

This prioritizes depth over breadth, meaning that it follows a single path of links as far as possible before backtracking and exploring other paths.

Useful for finding specific content or reaching deep into the structure.

**Extracting information**

It can extract a number of interesting data such as:
- Links
- Comments
- Metadata
- Sensitive files

Then after getting the data, they should be analyzed, for example if several URLs point to `/files/` and when you visit it manually you find out that directory browsing is enabled, thus exposing a host of files and backup archives.

Comments can have valuable informations as well, for example if you see a comment mentioning a `file server` and you combine with the information of the directory `/files/` you can think that the file server is publicly accessible.

# Robots.txt

It's like a guide for bots, outlining which areas of a website they are allowed to access and which not.

The directives in `robots.txt` targets user-agents, and it may look like:

```txt
User-agent: *
Disallow: /private/
```

This tells all user agents that they are not allowed to access any URLs starting with `/private/`.

While bots can still ignore it, most legitimate crawlers and search engine bots will respect it.

**Analyzing robots.txt**

```txt
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/

User-agent: Googlebot
Crawl-delay: 10

Sitemap: https://www.example.com/sitemap.xml
```

This file means:
- All user agents are disallowed to access `/admin/` and `/private/`.
- All are allowed to access `/public/`.
- The `Googlebot` is instructed to wait 10 seconds between requests.
- the `sitemap` file is provided for easier crawling.

# Well-Known URIs

The `/.well-known/` path on the server is a standard defined in `RFC 8615` and centralizes a website's critical metadata, including configuration files and information related to its services, protocols, and security mechanisms.

This directory simplifies the discovery and access process for various stakeholders, such as web browsers, apps, and security tools.

This allows clients to locate and retrieve specific config files, for example the security policy `https://website.com/.well-known/security.txt`.

The `Internet Assigned Numbers Authority - IANA` has a registry of `.well-known`.

In recon, this path can have very important information on discovering endpoints and config details that can be tested, for example `openid-configuration`.

When a client wants to used OpenID Connect for authentication, it can retrieve the config by accessing the `https://server/.well-known/openid-configuration`, which returns a JSON with metadata about the provider's endpoint, supported authentication methods, token issuance, etc:

```json
{
  "issuer": "https://example.com",
  "authorization_endpoint": "https://example.com/oauth2/authorize",
  "token_endpoint": "https://example.com/oauth2/token",
  "userinfo_endpoint": "https://example.com/oauth2/userinfo",
  "jwks_uri": "https://example.com/oauth2/jwks",
  "response_types_supported": ["code", "token", "id_token"],
  "subject_types_supported": ["public"],
  "id_token_signing_alg_values_supported": ["RS256"],
  "scopes_supported": ["openid", "profile", "email"]
}
```

# Creepy Crawlies

There are a lot of tools helping on crawling:
- Burp Suite Spider
- OWASP ZAP
- Scrapy
- Apache Nutch

**ReconSpider**
```sh
wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip
unzip ReconZiper.zip

python3 ReconSpider.py http://inlanefreight.com
```

# Search Engine Discovery

On search engines there are **search operators** that can be used to give a new level of precision and control and search for specific types of information.

|Operator|Description|
|--------|-----------|
|site:|limits results to a website|
|inurl:|finds pages with a term in the url|
|filetype:|searches for files of a type|
|intitle:|finds pages with a term in the title|
|cache:|displays cached version of a webpage|
|link:|finds pages that link to a specific webpage|

These are some of many operators that can be used.

# Web Archives

**Internet Archive's Wayback Machine** provides a unique opportunity to revisit the past and explore the digital footprints of websites as they once were.

It uses web crawlers to capture snapshots of websites at regular intervals automatically, stores the content of the pages.



